{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMMMZyJdoDeUuKIE8h9oOr2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["STEP 1: INSTALL DEPENDENCIES"],"metadata":{"id":"oPyAG3zv1KyL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YKRnk7A8uZk2"},"outputs":[],"source":["!pip install requests beautifulsoup4 numpy faiss-cpu sentence-transformers google-generativeai ipywidgets --quiet"]},{"cell_type":"markdown","source":["STEP 2: IMPORTS"],"metadata":{"id":"cYcVIhEcwaWr"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import re\n","import numpy as np\n","import faiss\n","from sentence_transformers import SentenceTransformer\n","import google.generativeai as genai\n","import ipywidgets as widgets\n","from IPython.display import display, Markdown\n","import os\n","import getpass\n","from urllib.parse import urljoin, urlparse\n","from collections import deque"],"metadata":{"id":"5a3DT0exwX6I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["STEP 3: SCRAPER WITH DEPTH AND PAGE LIMIT"],"metadata":{"id":"fYryXTP5wj_U"}},{"cell_type":"code","source":["def scrape_with_depth(start_urls, max_depth=1, max_pages=10):\n","    visited = set()\n","    texts = []\n","    queue = deque([(url, 0) for url in start_urls])\n","\n","    while queue and len(visited) < max_pages:\n","        url, depth = queue.popleft()\n","        if url in visited or depth > max_depth:\n","            continue\n","\n","        try:\n","            resp = requests.get(url, timeout=10)\n","            soup = BeautifulSoup(resp.content, 'html.parser')\n","\n","            # Extract text from multiple tags\n","            tags = soup.find_all(['p', 'li', 'td', 'th'])\n","            page_text = \"\\n\".join(tag.get_text(separator=\" \", strip=True) for tag in tags)\n","            texts.append(page_text)\n","            visited.add(url)\n","\n","            # Add links to queue if depth limit not reached\n","            if depth < max_depth:\n","                for link_tag in soup.find_all('a', href=True):\n","                    link = urljoin(url, link_tag['href'])\n","                    if urlparse(link).netloc == urlparse(url).netloc:  # stay on same domain\n","                        if link not in visited:\n","                            queue.append((link, depth + 1))\n","\n","        except Exception as e:\n","            print(f\"Error scraping {url}: {e}\")\n","\n","    return texts"],"metadata":{"id":"0kSqqyKnw-p7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["STEP 4: TEXT CLEANING"],"metadata":{"id":"q2c_25OrxJpZ"}},{"cell_type":"code","source":["def clean_text(text):\n","    text = re.sub(r'\\s+', ' ', text)\n","    text = re.sub(r'\\[[0-9]*\\]', '', text)\n","    text = re.sub(r'\\([^)]*\\)', '', text)\n","    return text.strip()\n","\n","def chunk_text(text, chunk_size=500):\n","    words = text.split()\n","    chunks = [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n","    return chunks"],"metadata":{"id":"goOOVSlIxIz4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["STEP 5: EMBEDDINGS & VECTOR BASE"],"metadata":{"id":"QHqDWDYHxaeX"}},{"cell_type":"code","source":["def build_vector_store(texts):\n","    cleaned_texts = [clean_text(t) for t in texts]\n","    all_chunks = []\n","    for text in cleaned_texts:\n","        all_chunks.extend(chunk_text(text))\n","\n","    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n","    embeddings = embedding_model.encode(all_chunks)\n","    index = faiss.IndexFlatL2(embeddings.shape[1])\n","    index.add(np.array(embeddings))\n","    return all_chunks, embedding_model, index\n","\n","def retrieve(query, embedding_model, index, all_chunks, top_k=3):\n","    query_embedding = embedding_model.encode([query])\n","    D, I = index.search(np.array(query_embedding), top_k)\n","    return [all_chunks[i] for i in I[0]]"],"metadata":{"id":"ZEaTlWmvxuO3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["STEP 6: WIDGETS UI"],"metadata":{"id":"qpowNhCwyAjt"}},{"cell_type":"code","source":["urls_input = widgets.Textarea(\n","    description=\"Websites:\",\n","    placeholder=\"Enter one or more URLs (each on a new line, e.g. https://www.who.int/news-room/fact-sheets/detail/vaccines-and-immunization)\",\n","    layout=widgets.Layout(width='500px', height='100px')\n",")\n","depth_input = widgets.IntText(value=1, description=\"Depth:\")\n","pages_input = widgets.IntText(value=10, description=\"Max Pages:\")\n","\n","query_input = widgets.Text(\n","    description=\"Your Claim:\",\n","    placeholder=\"e.g., Does garlic cure heart attacks?\",\n","    layout=widgets.Layout(width='500px')\n",")\n","scrape_button = widgets.Button(description='Scrape & Build Index', button_style='info')\n","generate_button = widgets.Button(description='Myth Buster Answer', button_style='success')\n","output = widgets.Output()\n","\n","# Variables to store vector DB\n","all_chunks = []\n","embedding_model = None\n","index = None"],"metadata":{"id":"oVMg2GkYyMdU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["STEP 7: BUTTON CALLBACKS\n"],"metadata":{"id":"KZkeAAp1ybm9"}},{"cell_type":"code","source":["def on_scrape_clicked(b):\n","    global all_chunks, embedding_model, index\n","    output.clear_output()\n","\n","    urls = [u.strip() for u in urls_input.value.strip().split(\"\\n\") if u.strip()]\n","    if not urls:\n","        with output:\n","            print(\"Please enter at least one website URL.\")\n","        return\n","\n","    with output:\n","        print(\"Scraping websites...\")\n","    texts = scrape_with_depth(urls, max_depth=depth_input.value, max_pages=pages_input.value)\n","    all_chunks, embedding_model, index = build_vector_store(texts)\n","\n","    with output:\n","        print(f\"Scraped and indexed {len(all_chunks)} chunks from {len(texts)} pages.\")\n","\n","def on_generate_clicked(b):\n","    global all_chunks, embedding_model, index\n","    output.clear_output()\n","\n","    if embedding_model is None or index is None:\n","        with output:\n","            print(\"Please scrape and build index first.\")\n","        return\n","\n","    query = query_input.value.strip()\n","    if not query:\n","        with output:\n","            print(\"Please enter a health claim.\")\n","        return\n","\n","    retrieved_chunks = retrieve(query, embedding_model, index, all_chunks, top_k=3)\n","    prompt = \"You are a health myth-busting expert. Using the following trusted information, answer the question truthfully and clearly, citing relevant context:\\n\\n\"\n","    for i, chunk in enumerate(retrieved_chunks):\n","        prompt += f\"Context {i+1}:\\n{chunk}\\n\\n\"\n","    prompt += f\"Question: {query}\\nAnswer:\"\n","\n","    try:\n","        response = gen_model.generate_content(prompt)\n","        answer = getattr(response, \"text\", \"\").strip() if hasattr(response, \"text\") else response.candidates[0].content.parts[0].text.strip()\n","    except Exception as e:\n","        answer = f\"Error calling Gemini API: {e}\"\n","\n","    with output:\n","        display(Markdown(f\"### Myth Buster Answer:\\n\\n{answer}\"))\n","\n","scrape_button.on_click(on_scrape_clicked)\n","generate_button.on_click(on_generate_clicked)"],"metadata":{"id":"EZhKfNe0ynIl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["STEP 8: DISPLAY UI & GEMINI KEY CONFIG\n","\n","\n","\n","\n"],"metadata":{"id":"MB0oo06fz7Rj"}},{"cell_type":"code","source":["os.environ['GOOGLE_API_KEY'] = getpass.getpass(\"Enter your Gemini API Key: \")\n","genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n","gen_model = genai.GenerativeModel(\"gemini-2.5-flash\")\n","\n","display(widgets.VBox([\n","    urls_input,\n","    depth_input,\n","    pages_input,\n","    scrape_button,\n","    query_input,\n","    generate_button,\n","    output\n","]))"],"metadata":{"id":"TEiyMfizypup"},"execution_count":null,"outputs":[]}]}