# health_myth_buster.py

import requests
from bs4 import BeautifulSoup
import re
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
import os
from urllib.parse import urljoin, urlparse
from collections import deque
import getpass

# ------------------------------
# STEP 1: SCRAPER WITH DEPTH & PAGE LIMIT
# ------------------------------
def scrape_with_depth(start_urls, max_depth=1, max_pages=10):
    visited = set()
    texts = []
    queue = deque([(url, 0) for url in start_urls])

    while queue and len(visited) < max_pages:
        url, depth = queue.popleft()
        if url in visited or depth > max_depth:
            continue

        try:
            resp = requests.get(url, timeout=10)
            soup = BeautifulSoup(resp.content, 'html.parser')

            # Extract text from multiple tags
            tags = soup.find_all(['p', 'li', 'td', 'th'])
            page_text = "\n".join(tag.get_text(separator=" ", strip=True) for tag in tags)
            texts.append(page_text)
            visited.add(url)

            # Add links to queue if depth limit not reached
            if depth < max_depth:
                for link_tag in soup.find_all('a', href=True):
                    link = urljoin(url, link_tag['href'])
                    if urlparse(link).netloc == urlparse(url).netloc:  # stay on same domain
                        if link not in visited:
                            queue.append((link, depth + 1))

        except Exception as e:
            print(f"Error scraping {url}: {e}")

    return texts

# ------------------------------
# STEP 2: TEXT CLEANING & CHUNKING
# ------------------------------
def clean_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\[[0-9]*\]', '', text)
    text = re.sub(r'\([^)]*\)', '', text)
    return text.strip()

def chunk_text(text, chunk_size=500):
    words = text.split()
    chunks = [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]
    return chunks

# ------------------------------
# STEP 3: EMBEDDINGS & VECTOR STORE
# ------------------------------
def build_vector_store(texts):
    cleaned_texts = [clean_text(t) for t in texts]
    all_chunks = []
    for text in cleaned_texts:
        all_chunks.extend(chunk_text(text))

    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = embedding_model.encode(all_chunks)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(np.array(embeddings))
    return all_chunks, embedding_model, index

def retrieve(query, embedding_model, index, all_chunks, top_k=3):
    query_embedding = embedding_model.encode([query])
    D, I = index.search(np.array(query_embedding), top_k)
    return [all_chunks[i] for i in I[0]]

# ------------------------------
# STEP 4: MAIN EXECUTION
# ------------------------------
def main():
    # Gemini API key setup
    api_key = getpass.getpass("Enter your Gemini API Key: ")
    os.environ['GOOGLE_API_KEY'] = api_key
    genai.configure(api_key=api_key)
    gen_model = genai.GenerativeModel("gemini-2.5-flash")

    # User input
    urls = input("Enter one or more URLs (comma-separated): ").split(",")
    urls = [u.strip() for u in urls if u.strip()]
    depth = int(input("Enter crawl depth (default 1): ") or 1)
    max_pages = int(input("Enter max pages to scrape (default 10): ") or 10)

    print("\nScraping websites...")
    texts = scrape_with_depth(urls, max_depth=depth, max_pages=max_pages)
    all_chunks, embedding_model, index = build_vector_store(texts)
    print(f"âœ… Scraped and indexed {len(all_chunks)} chunks from {len(texts)} pages.")

    # Query loop
    while True:
        query = input("\nEnter a health claim to check (or type 'exit' to quit): ").strip()
        if query.lower() == 'exit':
            print("Goodbye!")
            break

        retrieved_chunks = retrieve(query, embedding_model, index, all_chunks, top_k=3)
        prompt = "You are a health myth-busting expert. Using the following trusted information, answer the question truthfully and clearly, citing relevant context:\n\n"
        for i, chunk in enumerate(retrieved_chunks):
            prompt += f"Context {i+1}:\n{chunk}\n\n"
        prompt += f"Question: {query}\nAnswer:"

        try:
            response = gen_model.generate_content(prompt)
            answer = getattr(response, "text", "").strip() if hasattr(response, "text") else response.candidates[0].content.parts[0].text.strip()
        except Exception as e:
            answer = f"Error calling Gemini API: {e}"

        print("\nðŸ“¢ Myth Buster Answer:\n")
        print(answer)

if __name__ == "__main__":
    main()
